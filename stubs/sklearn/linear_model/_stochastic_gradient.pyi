import abc
from ..base import BaseEstimator as BaseEstimator, RegressorMixin as RegressorMixin, clone as clone, is_classifier as is_classifier
from ..exceptions import ConvergenceWarning as ConvergenceWarning
from ..model_selection import ShuffleSplit as ShuffleSplit, StratifiedShuffleSplit as StratifiedShuffleSplit
from ..utils import check_X_y as check_X_y, check_array as check_array, check_random_state as check_random_state, compute_class_weight as compute_class_weight, deprecated as deprecated
from ..utils.extmath import safe_sparse_dot as safe_sparse_dot
from ..utils.validation import check_is_fitted as check_is_fitted
from ._base import LinearClassifierMixin as LinearClassifierMixin, SparseCoefMixin as SparseCoefMixin, make_dataset as make_dataset
from ._sgd_fast import EpsilonInsensitive as EpsilonInsensitive, Hinge as Hinge, Huber as Huber, Log as Log, ModifiedHuber as ModifiedHuber, SquaredEpsilonInsensitive as SquaredEpsilonInsensitive, SquaredHinge as SquaredHinge, SquaredLoss as SquaredLoss
from abc import ABCMeta, abstractmethod
from typing import Any, Optional

LEARNING_RATE_TYPES: Any
PENALTY_TYPES: Any
DEFAULT_EPSILON: float
MAX_INT: Any

class _ValidationScoreCallback:
    estimator: Any = ...
    X_val: Any = ...
    y_val: Any = ...
    sample_weight_val: Any = ...
    def __init__(self, estimator: Any, X_val: Any, y_val: Any, sample_weight_val: Any, classes: Optional[Any] = ...) -> None: ...
    def __call__(self, coef: Any, intercept: Any): ...

class BaseSGD(SparseCoefMixin, BaseEstimator, metaclass=ABCMeta):
    loss: Any = ...
    penalty: Any = ...
    learning_rate: Any = ...
    epsilon: Any = ...
    alpha: Any = ...
    C: Any = ...
    l1_ratio: Any = ...
    fit_intercept: Any = ...
    shuffle: Any = ...
    random_state: Any = ...
    verbose: Any = ...
    eta0: Any = ...
    power_t: Any = ...
    early_stopping: Any = ...
    validation_fraction: Any = ...
    n_iter_no_change: Any = ...
    warm_start: Any = ...
    average: Any = ...
    max_iter: Any = ...
    tol: Any = ...
    def __init__(self, loss: Any, *, penalty: str = ..., alpha: float = ..., C: float = ..., l1_ratio: float = ..., fit_intercept: bool = ..., max_iter: int = ..., tol: float = ..., shuffle: bool = ..., verbose: int = ..., epsilon: float = ..., random_state: Optional[Any] = ..., learning_rate: str = ..., eta0: float = ..., power_t: float = ..., early_stopping: bool = ..., validation_fraction: float = ..., n_iter_no_change: int = ..., warm_start: bool = ..., average: bool = ...) -> None: ...
    def set_params(self, **kwargs: Any): ...
    @abstractmethod
    def fit(self, X: Any, y: Any) -> Any: ...
    @property
    def standard_coef_(self): ...
    @property
    def standard_intercept_(self): ...
    @property
    def average_coef_(self): ...
    @property
    def average_intercept_(self): ...

def fit_binary(est: Any, i: Any, X: Any, y: Any, alpha: Any, C: Any, learning_rate: Any, max_iter: Any, pos_weight: Any, neg_weight: Any, sample_weight: Any, validation_mask: Optional[Any] = ..., random_state: Optional[Any] = ...): ...

class BaseSGDClassifier(LinearClassifierMixin, BaseSGD, metaclass=ABCMeta):
    loss_functions: Any = ...
    class_weight: Any = ...
    n_jobs: Any = ...
    @abstractmethod
    def __init__(self, loss: str = ..., *, penalty: str = ..., alpha: float = ..., l1_ratio: float = ..., fit_intercept: bool = ..., max_iter: int = ..., tol: float = ..., shuffle: bool = ..., verbose: int = ..., epsilon: Any = ..., n_jobs: Optional[Any] = ..., random_state: Optional[Any] = ..., learning_rate: str = ..., eta0: float = ..., power_t: float = ..., early_stopping: bool = ..., validation_fraction: float = ..., n_iter_no_change: int = ..., class_weight: Optional[Any] = ..., warm_start: bool = ..., average: bool = ...) -> Any: ...
    def partial_fit(self, X: Any, y: Any, classes: Optional[Any] = ..., sample_weight: Optional[Any] = ...): ...
    def fit(self, X: Any, y: Any, coef_init: Optional[Any] = ..., intercept_init: Optional[Any] = ..., sample_weight: Optional[Any] = ...): ...

class SGDClassifier(BaseSGDClassifier):
    def __init__(self, loss: str = ..., *, penalty: str = ..., alpha: float = ..., l1_ratio: float = ..., fit_intercept: bool = ..., max_iter: int = ..., tol: float = ..., shuffle: bool = ..., verbose: int = ..., epsilon: Any = ..., n_jobs: Optional[Any] = ..., random_state: Optional[Any] = ..., learning_rate: str = ..., eta0: float = ..., power_t: float = ..., early_stopping: bool = ..., validation_fraction: float = ..., n_iter_no_change: int = ..., class_weight: Optional[Any] = ..., warm_start: bool = ..., average: bool = ...) -> None: ...
    @property
    def predict_proba(self): ...
    @property
    def predict_log_proba(self): ...

class BaseSGDRegressor(RegressorMixin, BaseSGD, metaclass=abc.ABCMeta):
    loss_functions: Any = ...
    @abstractmethod
    def __init__(self, loss: str = ..., *, penalty: str = ..., alpha: float = ..., l1_ratio: float = ..., fit_intercept: bool = ..., max_iter: int = ..., tol: float = ..., shuffle: bool = ..., verbose: int = ..., epsilon: Any = ..., random_state: Optional[Any] = ..., learning_rate: str = ..., eta0: float = ..., power_t: float = ..., early_stopping: bool = ..., validation_fraction: float = ..., n_iter_no_change: int = ..., warm_start: bool = ..., average: bool = ...) -> Any: ...
    def partial_fit(self, X: Any, y: Any, sample_weight: Optional[Any] = ...): ...
    def fit(self, X: Any, y: Any, coef_init: Optional[Any] = ..., intercept_init: Optional[Any] = ..., sample_weight: Optional[Any] = ...): ...
    def predict(self, X: Any): ...

class SGDRegressor(BaseSGDRegressor):
    def __init__(self, loss: str = ..., *, penalty: str = ..., alpha: float = ..., l1_ratio: float = ..., fit_intercept: bool = ..., max_iter: int = ..., tol: float = ..., shuffle: bool = ..., verbose: int = ..., epsilon: Any = ..., random_state: Optional[Any] = ..., learning_rate: str = ..., eta0: float = ..., power_t: float = ..., early_stopping: bool = ..., validation_fraction: float = ..., n_iter_no_change: int = ..., warm_start: bool = ..., average: bool = ...) -> None: ...
